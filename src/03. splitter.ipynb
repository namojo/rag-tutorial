{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain text_generation faiss-cpu pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래에서 계속 사용할 LLM을 만들어줍니다. \n",
    "\n",
    "import warnings\n",
    "\n",
    "from langchain_community.llms import HuggingFaceTextGenInference\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 야놀자에서 만든 EEVE Korean 8B 모델을 사용합니다.\n",
    "llm = HuggingFaceTextGenInference(\n",
    "    inference_server_url=\"http://eeve-korean-instruct-10-8b-tgi.serving.70-220-152-1.sslip.io\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='2024년 2월호', metadata={'source': '../data/AI_Brief.pdf', 'page': 0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF 파일 로드. 파일의 경로 입력\n",
    "loader = PyPDFLoader(\"../data/AI_Brief.pdf\")\n",
    "\n",
    "# 페이지 별 문서 로드\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 19\n"
     ]
    }
   ],
   "source": [
    "print(f\"문서의 수: {len(pages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024년 2월호\n",
      "{'source': '../data/AI_Brief.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 문서의 내용 출력\n",
    "print(pages[0].page_content)\n",
    "\n",
    "\n",
    "# 첫번째 문서의 메타데이터 출력\n",
    "print(pages[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q pyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='2024년 2월호\\n', metadata={'source': '../data/AI_Brief.pdf', 'file_path': '../data/AI_Brief.pdf', 'page': 0, 'total_pages': 20, 'format': 'PDF 1.4', 'title': '', 'author': 'nipa', 'subject': '', 'keywords': '', 'creator': 'Hwp 2018 10.0.0.13015', 'producer': 'Hancom PDF 1.3.0.542', 'creationDate': \"D:20240206111338+09'00'\", 'modDate': \"D:20240206111338+09'00'\", 'trapped': ''})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# PDF 파일 로드. 파일의 경로 입력\n",
    "loader = PyMuPDFLoader(\"../data/AI_Brief.pdf\")\n",
    "\n",
    "# 페이지 별 문서 로드\n",
    "data = loader.load()\n",
    "\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='2024년 2월호\\nⅠ. 인공지능 산업 동향 브리프\\n 1. 정책/법제 \\n   ▹ OECD, 금융 분야의 생성 AI 위험과 정책 고려사항을 다룬 보고서 발간························· 1\\n   ▹ 일본 AI 전략회의, AI의 안전한 활용을 위한 AI 사업자 가이드라인 초안 공개  ··········· 2\\n   ▹ 영국 대법원, AI는 특허 발명자가 될 수 없다고 최종 판결 ·············································· 3\\n   ▹ 문화체육관광부와 한국저작권위원회, ‘생성형 AI 저작권 안내서’ 발간  ··························· 4\\n \\n 2. 기업/산업 \\n   ▹ 오픈AI, 유료 사용자 대상 GPT 스토어 출시······································································· 5\\n   ▹ 업스테이지, 콴다와 수학 특화 AI 모델 공동 개발  ·························································· 6\\n   ▹ 뉴욕타임스, 저작권 침해로 오픈AI와 마이크로소프트 고소   ··········································· 7\\n   ▹ 오픈AI, 생성 AI를 이용한 선거 개입을 막기 위한 대응책 공개  ····································· 8\\n   ▹ CES 2024, 컴패니언 로봇과 일상용품에 이르기까지 다양한 AI 제품 공개 ··················· 9\\n 3. 기술/연구\\n   ▹ 구글 딥마인드, AI 모델을 활용한 자율로봇 기술 ‘오토RT’ 공개····································· 1\\n0\\n   ▹ 구글 딥마인드, 올림피아드 수준의 기하학 특화 AI ‘알파지오메트리’ 공개 ··················· 1\\n1\\n   ▹ 맥아피, 딥페이크 음성 판독하는 AI 기술 ‘프로젝트 모킹버드’ 공개 ····························· 1\\n2\\n   \\n 4. 인력/교육     \\n   ▹ IDC, 2027년까지 생성 AI가 마케팅 업무의 30% 대체 전망········································· 1\\n3\\n   ▹ 한국언론진흥재단 조사, 언론인 절반 이상이 직무에 생성 AI 활용·································· 1\\n4\\n \\nⅡ. 주요 행사\\n  ▹Microsoft 365 Conference ································································································· 1\\n5\\n  ▹World Summit AI Americas ······························································································ 1\\n5\\n  ▹AIAI ········································································································································ 1\\n5\\n', metadata={'source': '../data/AI_Brief.pdf', 'file_path': '../data/AI_Brief.pdf', 'page': 1, 'total_pages': 20, 'format': 'PDF 1.4', 'title': '', 'author': 'nipa', 'subject': '', 'keywords': '', 'creator': 'Hwp 2018 10.0.0.13015', 'producer': 'Hancom PDF 1.3.0.542', 'creationDate': \"D:20240206111338+09'00'\", 'modDate': \"D:20240206111338+09'00'\", 'trapped': ''})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "프로젝트 모킹버드’ 공개 ····························· 1\n",
      "2\n",
      "   \n",
      " 4. 인력/교육     \n",
      "   ▹ IDC, 2027년까지 생성 AI가 마케팅 업무의 30% 대체 전망········································· 1\n",
      "3\n",
      "   ▹ 한국언론진흥재단 조사, 언론인 절반 이상이 직무에 생성 AI 활용·································· 1\n",
      "4\n",
      " \n",
      "Ⅱ. 주요 행사\n",
      "  ▹Microsoft 365 Conference ································································································· 1\n",
      "5\n",
      "  ▹World Summit AI Americas ······························································································ 1\n",
      "5\n",
      "  ▹AIAI ········································································································································ 1\n",
      "5\n",
      "Ⅰ. 인공지능 산업 동향 브리프\n",
      "1. 정책/법제  \n",
      "2. 기업/산업 \n",
      "3. 기술/연구 \n",
      " 4. 인력/교육\n",
      "OECD, 금융 분야 생성 AI 위험과 정책 고려사항을 다룬 보고서 발간\n",
      "n OECD가 발간한 금융 분야 생성 AI 보고서에 따르면 생성 AI 도입으로 금융 분야의 \n",
      "효율성이 높아지는 한편, 다양한 위험도 제기될 수 있음\n",
      "n OECD는 금융 분야에서 생성 AI 도입으로 인한 위험을 막기 위한 정책 \n",
      "고려사항으로 모델 거버넌스 강화와 인간 중심의 접근방식을 강조\n",
      "KEY Contents\n",
      "£ 금융 분야의 생성 AI 도입 확산 시 편견과 차별, 데이터 침해 등 다양한 위험 발생 가능\n",
      "n OECD가 2023년 12월 15\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "\n",
    "# 파일열기\n",
    "doc = fitz.open(\"../data/AI_Brief.pdf\")\n",
    "\n",
    "# 페이지별로 문서를 읽어오면서 하나의 문자열에 append 하여 결합\n",
    "texts = \"\"\n",
    "for page in doc:\n",
    "    texts += page.get_text()\n",
    "\n",
    "# 앞의 200 글자만 출력\n",
    "print(texts[1000:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is the text I would like to chunk up. It is the example text for this exercise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the text I would like to ch',\n",
       " 'unk up. It is the example text for ',\n",
       " 'this exercise']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list that will hold your chunks\n",
    "chunks = []\n",
    "\n",
    "chunk_size = 35 # Characters\n",
    "\n",
    "# Run through the a range with the length of your text and iterate every chunk_size you want\n",
    "for i in range(0, len(text), chunk_size):\n",
    "    chunk = text[i:i + chunk_size]\n",
    "    chunks.append(chunk)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"안녕하세요. RAG프로그래밍에 오신걸 환영합니다. Langchain Docs를 기반으로 만들어졌습니다. 안녕하세요. RAG프로그래밍에 오신걸 환영합니다. Langchain Docs를 기반으로 만들어졌습니다. 안녕하세요. RAG프로그래밍에 오신걸 환영합니다. Langchain Docs를 기반으로 만들어졌습니다.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕하세요. RAG프로그래밍에 오신걸 환영합니다',\n",
       " ' Langchain Docs를 기반으로 만들어졌습니다',\n",
       " ' 안녕하세요. RAG프로그래밍에 오신걸 환영합니다',\n",
       " ' Langchain Docs를 기반으로 만들어졌습니다',\n",
       " ' 안녕하세요. RAG프로그래밍에 오신걸 환영합니다',\n",
       " ' Langchain Docs를 기반으로 만들어졌습니다']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=35, chunk_overlap=0, separator='.', strip_whitespace=False)\n",
    "\n",
    "# 35글자 제한과 \".\"을 구분자로 해서 chunk 단위를 나눕니다.\n",
    "\n",
    "text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕하세요. RAG프로그래밍에 오신걸 환영합니다.',\n",
       " '오신걸 환영합니다. Langchain Docs를 기반으로',\n",
       " 'Docs를 기반으로 만들어졌습니다. 안녕하세요.',\n",
       " '안녕하세요. RAG프로그래밍에 오신걸 환영합니다.',\n",
       " '오신걸 환영합니다. Langchain Docs를 기반으로',\n",
       " 'Docs를 기반으로 만들어졌습니다. 안녕하세요.',\n",
       " '안녕하세요. RAG프로그래밍에 오신걸 환영합니다.',\n",
       " '오신걸 환영합니다. Langchain Docs를 기반으로',\n",
       " 'Docs를 기반으로 만들어졌습니다.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=35, chunk_overlap=10, separator=\" \")\n",
    "\n",
    "# \" \" (공백)을 구분자로 해서 나누는 방법도 있죠.\n",
    "\n",
    "text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharacterTextSplitter \t\t 사용시 문서의 수: 19\n"
     ]
    }
   ],
   "source": [
    "# 그렇다면 불러온 PDF 문서를 Character splitter로 분할해봅시다.\n",
    "\n",
    "# splitter 정의\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "# 문서 로드 및 분할 (load_and_split)\n",
    "split_docs = loader.load_and_split(text_splitter=text_splitter)\n",
    "print(f\"CharacterTextSplitter \\t\\t 사용시 문서의 수: {len(split_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. 정책/법제  2. 기업/산업 3. 기술/연구  4. 인력/교육\\nOECD, 금융 분야 생성 AI 위험과 정책 고려사항을 다룬 보고서 발간\\nnOECD 가 발간한 금융 분야 생성 AI 보고서에 따르면 생성 AI 도입으로 금융 분야의 \\n효율성이 높아지는 한편, 다양한 위험도 제기될 수 있음\\nnOECD 는 금융 분야에서 생성 AI 도입으로 인한 위험을 막기 위한 정책 \\n고려사항으로 모델 거버넌스 강화와 인간 중심의 접근방식을 강조KEY Contents\\n£금융 분야의 생성 AI 도입 확산 시 편견과 차별, 데이터 침해 등 다양한 위험 발생 가능\\nnOECD 가 2023년 12월 15일 ‘금융 분야 생성 AI(Generative Artificial Intelligence In \\nFinance)’ 보고서를 발간하고 금융 분야에서 생성 AI 도입 현황을 개괄\\n∙사용자 친화적이고 직관적인 인터페이스를 가진 생성 AI 등장 이후에도 금융 분야에서 AI는 금융사의 \\n운영 효율성을 높이기 위한 업무 자동화를 중심으로 활용되며 , 인간의 개입 없는 완전 자동화와 같은 \\n고급 활용 사례는 개발 단계에 머물러 있음\\n∙금융 분야에서 생성 AI 도입 속도가 느린 이유는 소비자 보호와 금융안전을 보장하기 위한 엄격한 \\n규제의 적용을 받기 때문으로 , 향후 생성 AI 기술이 발전하면서 고객 지원과 신제품 개발, 제품 기능 \\n최적화 등 다양한 영역에서 활용 전망\\nn금융 분야에서 생성 AI 도입이 늘어날 경우, 차별이나 불공정성 , 개인정보 및 지식재산권 침해 등 \\n기존에 존재하던 위험이 증폭되는 한편 새로운 위험 요소도 등장할 수 있음\\n∙AI 모델은 품질이 낮거나 부적절한 데이터셋을 학습하여 금융 대출 시 차별적이거나 불공정한 결과를 \\n초래할 수 있으며 , 개인정보 침해나 지식재산권 침해의 가능성도 존재\\n∙AI 모델이 결과를 생성하는 이유와 방법에 대한 설명 가능성의 부족으로 금융서비스의 신뢰 수준이 \\n저하될 수 있으며 , 악의적 행위자가 생성 AI를 악용해 금융 시장을 조작할 수도 있음\\n£금융 분야에서 생성 AI로 인한 위험 완화를 위한 정책적 대응 필요\\nn정 책  입 안 자 는  혁 신 을  저 해 하 지  않 으 면 서  책 임  있 고  안 전 한  AI 도 입 을  위 해  금 융  분 야 에 서  생 성  A I로 \\n인한 위험을 파악하고 완화할 수 있는 정책을 고려해야 함 \\n∙편견과 차별의 위험을 막기 위한 보호조치를 마련하는 한편, 생성 AI 모델의 설명 가능성과 투명성을 \\n향상하려는 노력을 고취하고 , AI 모델에 대한 거버넌스를 강화하여 모델 개발과 배포와 관련된 \\n당사자에게 책임을 지도록 요구해야 함\\n∙대출과 같이 중요한 의사결정에서 인간 중심의 원칙을 장려하고 , 인간의 개입을 요청하거나 모델의 \\n결과에 이의를 제기하고 시정을 요구할 권리를 고객에게 부여하는 방안을 고려  \\n☞ 출처: OECD, Generative artificial intelligence in finance, 2023.12.15.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs[3].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecursiveCharacterTextSplitter \t 사용시 문서의 수: 34\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"],\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# 문서 로드 및 분할 (load_and_split)\n",
    "split_doc = loader.load_and_split(text_splitter=text_splitter)\n",
    "print(f\"RecursiveCharacterTextSplitter \\t 사용시 문서의 수: {len(split_doc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.\n",
    "with open(\"../data/appendix-keywords.txt\") as f:\n",
    "    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\n",
      "예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\n",
      "연관키워드: 토큰화, 자연어\n"
     ]
    }
   ],
   "source": [
    "# 파일으로부터 읽은 내용을 일부 출력합니다.\n",
    "print(file[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # 청크 크기를 매우 작게 설정합니다. 예시를 위한 설정입니다.\n",
    "    chunk_size=250,\n",
    "    # 청크 간의 중복되는 문자 수를 설정합니다.\n",
    "    chunk_overlap=50,\n",
    "    # 문자열 길이를 계산하는 함수를 지정합니다.\n",
    "    length_function=len,\n",
    "    # 구분자로 정규식을 사용할지 여부를 설정합니다.\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Semantic Search\\n\\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\\n예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\\n\\nEmbedding'\n",
      "============================================================\n",
      "page_content='Embedding\\n\\n정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\\n예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\\n연관키워드: 자연어 처리, 벡터화, 딥러닝\\n\\nToken'\n"
     ]
    }
   ],
   "source": [
    "# text_splitter를 사용하여 file 텍스트를 문서로 분할합니다.\n",
    "texts = text_splitter.create_documents([file])\n",
    "print(texts[0])  # 분할된 문서의 첫 번째 문서를 출력합니다.\n",
    "print(\"===\" * 20)\n",
    "print(texts[1])  # 분할된 문서의 두 번째 문서를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain_experimental "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomEmbedding (SDS임베딩)을 사용합니다. \n",
    "# 로컬에서 사용할 때는 import 해주세요. \n",
    "\n",
    "from typing import (List)\n",
    "\n",
    "import requests\n",
    "from langchain_core.embeddings import Embeddings\n",
    "\n",
    "\n",
    "class CustomEmbedding(Embeddings):\n",
    "    def __init__(self):\n",
    "        self.embed_url = 'http://sds-embed.serving.70-220-152-1.sslip.io/v1/models/embed:predict'\n",
    "\n",
    "    def call_embed(self, url, texts):\n",
    "        data = {\n",
    "            \"instances\": texts\n",
    "        }\n",
    "        response = requests.post(url=url, json=data)\n",
    "        result = response.json()\n",
    "        return result['predictions']\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        주어진 텍스트를 임베딩하여 벡터로 반환 합니다.\n",
    "        \"\"\"\n",
    "        embed_list = self.call_embed(url=self.embed_url, texts=texts)\n",
    "        return embed_list\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed query text.\"\"\"\n",
    "\n",
    "        embed_list = self.call_embed(url=self.embed_url, texts=[text])\n",
    "        return embed_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.\n",
    "with open(\"../data/state_of_the_union.txt\") as f:\n",
    "    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SemanticChunker(CustomEmbedding())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s\n"
     ]
    }
   ],
   "source": [
    "# SDS 임베딩과 OpenAI 임베딩의 차이 때문에 Semantic Chunking이 정상동작하지 않습니다. \n",
    "\n",
    "docs = text_splitter.create_documents(file)\n",
    "print(docs[55].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
