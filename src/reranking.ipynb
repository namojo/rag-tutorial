{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG 시스템에서의 Reranking 결정 방법\n",
    "\n",
    "## 개요\n",
    "Reranking은 검색-증강 생성(RAG) 시스템에서 중요한 단계로, 검색된 문서의 관련성과 품질을 개선하는 것을 목표로 합니다. 그것은 초기에 검색된 문서를 재평가하고 재정렬하여 후속 처리 또는 제시를 위해 가장 적절한 정보를 우선시하는 것을 포함합니다.\n",
    "\n",
    "## 필요성\n",
    "RAG 시스템의 경우, Reranking의 주요 동기는 초기 검색 방법의 한계를 극복하는 것으로, 이는 종종 더 단순한 유사성 메트릭에 의존합니다. Reranking은 기존 검색 기술에서 놓칠 수 있는 질의와 문서 간의 미묘한 관계를 고려하여 보다 정교한 관련성 평가를 가능하게 합니다. 이 프로세스는 생성 단계에서 가장 적절한 정보가 사용되도록 하여 RAG 시스템의 전반적인 성능을 향상시키는 것을 목표로 합니다.\n",
    "\n",
    "## 주요 구성 요소\n",
    "Reranking 시스템에는 일반적으로 다음 구성 요소가 포함됩니다.\n",
    "\n",
    "1. 초기 검색기: 종종 임베딩 기반 유사도 검색을 사용하는 벡터 저장소입니다.\n",
    "2. Reranking 모델: 이것은 둘 중 하나일 수 있습니다.\n",
    "   - 관련성 평가를 위해 특별히 훈련된 대형 언어 모델(LLM)\n",
    "   - 관련성 평가용으로 특별히 훈련된 교차 인코더 모델\n",
    "3. 점수 매기기 메커니즘: 문서에 관련성 점수를 할당하기 위한 방법입니다.\n",
    "4. 정렬 및 선택 논리: 새 점수에 따라 문서를 다시 정렬합니다.\n",
    "\n",
    "## 방법 세부 정보\n",
    "Reranking 프로세스는 일반적으로 이러한 단계를 따릅니다.\n",
    "\n",
    "1. 초기 검색: 잠재적으로 관련된 초기 세트의 문서를 가져옵니다.\n",
    "2. 페어 생성: 각 검색된 문서에 대해 쿼리-문서 쌍을 만듭니다.\n",
    "3. 채점:\n",
    "   - LLM 방법: 프롬프트를 사용하여 LLM에게 문서 관련 등급을 묻습니다.\n",
    "   - 교차 인코더 방법: 쿼리-문서 쌍을 직접 모델에 공급합니다.\n",
    "4. 점수 해석: 관련성 점수를 파싱하고 정규화합니다.\n",
    "5. 순서 변경: 새로운 관련성 점수에 따라 문서를 정렬합니다.\n",
    "6. 선택: 재정렬 목록에서 상위 K개의 문서를 선택합니다.\n",
    "\n",
    "## 이 접근법의 이점\n",
    "Reranking은 여러 가지 장점을 제공합니다.\n",
    "\n",
    "1. 관련성 개선: 보다 정교한 모델을 사용하여 Reranking은 미묘한 관련성 요소를 포착할 수 있습니다.\n",
    "2. 유연성: 특정 요구 사항 및 리소스에 따라 다른 Reranking 방법을 적용할 수 있습니다.\n",
    "3. 향상된 컨텍스트 품질: RAG 시스템에 보다 관련성이 높은 문서를 제공함으로써 생성된 응답의 품질이 향상됩니다.\n",
    "4. 노이즈 감소: Reranking은 덜 관련성이 있는 정보를 필터링하는 데 도움이 되며 가장 관련성이 높은 콘텐츠에 초점을 맞춥니다.\n",
    "\n",
    "## 결론\n",
    "Reranking은 RAG 시스템에서 강력한 기술로 검색된 정보의 품질을 크게 향상시킵니다. LLM 기반 점수 매기기 또는 특수화된 크로스 인코더 모델을 사용하는지 여부에 관계없이, Reranking은 문서 관련성의 미묘한 차이를 허용합니다. 이러한 개선된 관련성은 다운스트림 작업의 성능 향상으로 직결되므로, Reranking은 고급 RAG 구현에서 필수적인 구성 요소가 됩니다.\n",
    "\n",
    "LLM 기반과 교차 인코더 Reranking 방법 사이의 선택은 필요한 정확도, 사용 가능한 계산 리소스 및 특정 애플리케이션 요구 사항과 같은 요인에 따라 달라집니다. 두 가지 접근 방식 모두 기본 검색 방법보다 실질적인 개선을 제공하며 RAG 시스템의 전반적인 효과성에 기여합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "#from dotenv import load_dotenv\n",
    "from langchain.docstore.document import Document\n",
    "from typing import List, Dict, Any, Tuple\n",
    "#from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add the parent directory to the path sicnce we work with notebooks\n",
    "#from helper_functions import *\n",
    "#from evaluation.evalute_rag import *\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "#load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "#os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the document's path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/Understanding_Climate_Change.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = encode_pdf(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: LLM based function to rerank the retrieved documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<img src=\"../images/rerank_llm.svg\" alt=\"rerank llm\" style=\"width:40%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom reranking function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatingScore(BaseModel):\n",
    "    relevance_score: float = Field(..., description=\"The relevance score of a document to a query.\")\n",
    "\n",
    "def rerank_documents(query: str, docs: List[Document], top_n: int = 3) -> List[Document]:\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"query\", \"doc\"],\n",
    "        template=\"\"\"On a scale of 1-10, rate the relevance of the following document to the query. Consider the specific context and intent of the query, not just keyword matches.\n",
    "        Query: {query}\n",
    "        Document: {doc}\n",
    "        Relevance Score:\"\"\"\n",
    "    )\n",
    "        \n",
    "    llm_chain = prompt_template | llm.with_structured_output(RatingScore)\n",
    "        \n",
    "    scored_docs = []\n",
    "    for doc in docs:\n",
    "        input_data = {\"query\": query, \"doc\": doc.page_content}\n",
    "        score = llm_chain.invoke(input_data).relevance_score\n",
    "        try:\n",
    "            score = float(score)\n",
    "        except ValueError:\n",
    "            score = 0  # Default score if parsing fails\n",
    "        scored_docs.append((doc, score))\n",
    "    \n",
    "    reranked_docs = sorted(scored_docs, key=lambda x: x[1], reverse=True)\n",
    "    return [doc for doc, _ in reranked_docs[:top_n]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usage of the reranking function with a sample query relevant to the document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWhat are the impacts of climate change on biodiversity?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m initial_docs \u001b[39m=\u001b[39m vectorstore\u001b[39m.\u001b[39msimilarity_search(query, k\u001b[39m=\u001b[39m\u001b[39m15\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m reranked_docs \u001b[39m=\u001b[39m rerank_documents(query, initial_docs)\n\u001b[1;32m      5\u001b[0m \u001b[39m# print first 3 initial documents\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTop initial documents:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[67], line 13\u001b[0m, in \u001b[0;36mrerank_documents\u001b[0;34m(query, docs, top_n)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrerank_documents\u001b[39m(query: \u001b[39mstr\u001b[39m, docs: List[Document], top_n: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[1;32m      5\u001b[0m     prompt_template \u001b[39m=\u001b[39m PromptTemplate(\n\u001b[1;32m      6\u001b[0m         input_variables\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdoc\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m      7\u001b[0m         template\u001b[39m=\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39mOn a scale of 1-10, rate the relevance of the following document to the query. Consider the specific context and intent of the query, not just keyword matches.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39m        Relevance Score:\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     11\u001b[0m     )\n\u001b[0;32m---> 13\u001b[0m     llm_chain \u001b[39m=\u001b[39m prompt_template \u001b[39m|\u001b[39m llm\u001b[39m.\u001b[39;49mwith_structured_output(RatingScore)\n\u001b[1;32m     15\u001b[0m     scored_docs \u001b[39m=\u001b[39m []\n\u001b[1;32m     16\u001b[0m     \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain_core/language_models/base.py:239\u001b[0m, in \u001b[0;36mBaseLanguageModel.with_structured_output\u001b[0;34m(self, schema, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Not implemented on this class.\"\"\"\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[39m# Implement this on child class if there is a way of steering the model to\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[39m# generate responses that match a given schema.\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m()\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "query = \"What are the impacts of climate change on biodiversity?\"\n",
    "initial_docs = vectorstore.similarity_search(query, k=15)\n",
    "reranked_docs = rerank_documents(query, initial_docs)\n",
    "\n",
    "# print first 3 initial documents\n",
    "print(\"Top initial documents:\")\n",
    "for i, doc in enumerate(initial_docs[:3]):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content[:200] + \"...\")  # Print first 200 characters of each document\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Top reranked documents:\")\n",
    "for i, doc in enumerate(reranked_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content[:200] + \"...\")  # Print first 200 characters of each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom retriever based on our reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom retriever class\n",
    "class CustomRetriever(BaseRetriever, BaseModel):\n",
    "    \n",
    "    vectorstore: Any = Field(description=\"Vector store for initial retrieval\")\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def get_relevant_documents(self, query: str, num_docs=2) -> List[Document]:\n",
    "        initial_docs = self.vectorstore.similarity_search(query, k=30)\n",
    "        return rerank_documents(query, initial_docs, top_n=num_docs)\n",
    "\n",
    "\n",
    "# Create the custom retriever\n",
    "custom_retriever = CustomRetriever(vectorstore=vectorstore)\n",
    "\n",
    "# Create an LLM for answering questions\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
    "\n",
    "# Create the RetrievalQA chain with the custom retriever\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=custom_retriever,\n",
    "    return_source_documents=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "print(f\"\\nQuestion: {query}\")\n",
    "print(f\"Answer: {result['result']}\")\n",
    "print(\"\\nRelevant source documents:\")\n",
    "for i, doc in enumerate(result[\"source_documents\"]):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content[:200] + \"...\")  # Print first 200 characters of each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example that demonstrates why we should use reranking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of Retrieval Techniques\n",
      "==================================\n",
      "Query: what is the capital of france?\n",
      "\n",
      "Baseline Retrieval Result:\n",
      "\n",
      "Document 1:\n",
      "The capital of France is great.\n",
      "\n",
      "Document 2:\n",
      "The capital of France is beautiful.\n",
      "\n",
      "Advanced Retrieval Result:\n",
      "\n",
      "Document 1:\n",
      "I really enjoyed my trip to Paris, France. The city is beautiful and the food is delicious. I would love to visit again. Such a great capital city.\n",
      "\n",
      "Document 2:\n",
      "Have you ever visited Paris? It is a beautiful city where you can eat delicious food and see the Eiffel Tower. \n",
      "    I really enjoyed all the cities in france, but its capital with the Eiffel Tower is my favorite city.\n"
     ]
    }
   ],
   "source": [
    "chunks = [\n",
    "    \"The capital of France is great.\",\n",
    "    \"The capital of France is huge.\",\n",
    "    \"The capital of France is beautiful.\",\n",
    "    \"\"\"Have you ever visited Paris? It is a beautiful city where you can eat delicious food and see the Eiffel Tower. \n",
    "    I really enjoyed all the cities in france, but its capital with the Eiffel Tower is my favorite city.\"\"\", \n",
    "    \"I really enjoyed my trip to Paris, France. The city is beautiful and the food is delicious. I would love to visit again. Such a great capital city.\"\n",
    "]\n",
    "docs = [Document(page_content=sentence) for sentence in chunks]\n",
    "\n",
    "\n",
    "def compare_rag_techniques(query: str, docs: List[Document] = docs) -> None:\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "    print(\"Comparison of Retrieval Techniques\")\n",
    "    print(\"==================================\")\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    \n",
    "    print(\"Baseline Retrieval Result:\")\n",
    "    baseline_docs = vectorstore.similarity_search(query, k=2)\n",
    "    for i, doc in enumerate(baseline_docs):\n",
    "        print(f\"\\nDocument {i+1}:\")\n",
    "        print(doc.page_content)\n",
    "\n",
    "    print(\"\\nAdvanced Retrieval Result:\")\n",
    "    custom_retriever = CustomRetriever(vectorstore=vectorstore)\n",
    "    advanced_docs = custom_retriever.get_relevant_documents(query)\n",
    "    for i, doc in enumerate(advanced_docs):\n",
    "        print(f\"\\nDocument {i+1}:\")\n",
    "        print(doc.page_content)\n",
    "\n",
    "\n",
    "query = \"what is the capital of france?\"\n",
    "compare_rag_techniques(query, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Cross Encoder models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<img src=\"../images/rerank_cross_encoder.svg\" alt=\"rerank cross encoder\" style=\"width:40%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the cross encoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "class CrossEncoderRetriever(BaseRetriever, BaseModel):\n",
    "    vectorstore: Any = Field(description=\"Vector store for initial retrieval\")\n",
    "    cross_encoder: Any = Field(description=\"Cross-encoder model for reranking\")\n",
    "    k: int = Field(default=5, description=\"Number of documents to retrieve initially\")\n",
    "    rerank_top_k: int = Field(default=3, description=\"Number of documents to return after reranking\")\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # Initial retrieval\n",
    "        initial_docs = self.vectorstore.similarity_search(query, k=self.k)\n",
    "        \n",
    "        # Prepare pairs for cross-encoder\n",
    "        pairs = [[query, doc.page_content] for doc in initial_docs]\n",
    "        \n",
    "        # Get cross-encoder scores\n",
    "        scores = self.cross_encoder.predict(pairs)\n",
    "        \n",
    "        # Sort documents by score\n",
    "        scored_docs = sorted(zip(initial_docs, scores), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top reranked documents\n",
    "        return [doc for doc, _ in scored_docs[:self.rerank_top_k]]\n",
    "\n",
    "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        raise NotImplementedError(\"Async retrieval not implemented\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an instance and showcase over an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the cross-encoder retriever\n",
    "cross_encoder_retriever = CrossEncoderRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    cross_encoder=cross_encoder,\n",
    "    k=10,  # Retrieve 10 documents initially\n",
    "    rerank_top_k=5  # Return top 5 after reranking\n",
    ")\n",
    "\n",
    "# Set up the LLM\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
    "\n",
    "# Create the RetrievalQA chain with the cross-encoder retriever\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=cross_encoder_retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Example query\n",
    "query = \"What are the impacts of climate change on biodiversity?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "print(f\"\\nQuestion: {query}\")\n",
    "print(f\"Answer: {result['result']}\")\n",
    "print(\"\\nRelevant source documents:\")\n",
    "for i, doc in enumerate(result[\"source_documents\"]):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content[:200] + \"...\")  # Print first 200 characters of each document"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
